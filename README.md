# ACT-SR: Aggregation Connection Transformer for Remote Sensing Image Super-Resolution
Official Pytorch implementation of the paper "[ACT-SR: Aggregation Connection Transformer for Remote Sensing Image Super-Resolution]

Recently, Transformer-based methods have shown impressive performances in remote sensing image super resolu- tion (RSISR). However, the application of Transformer in RSISR frequently results in artifacts and the loss of image detail due to limited information transmission pathways and the constraints of uni-dimensional self-attention mechanisms. To solve these problems, an Aggregation Connection Transformer (ACT-SR) is proposed for RSISR. ACT-SR employs an advanced attention mechanism designed to enrich information transmission across spatial and channel dimensions, thereby enlarging the receptive fields for more accurate feature extraction. A core component of ACT-SR is the novel aggregation connection Transformer Group (ACTG), which effectively captures spatial similarities and channel importance and aggregate these information using a combination of series and parallel connection for enhanced feature representation. Furthermore, a new gated feed-forward network (GFN) is introduced to enhance the nonlinear mapping capabilities of the Transformer and control the information flow through the network. In addition, ACT-SR integrates a shift windows scheme alongside residual learning to facilitate efficient detail recovery and artifact elimination. Experimental results confirm the effectiveness of the proposed modules, with ACT-SR outperforming several state-of-the-art RSISR methods in both objective metrics and visual quality.

## Result
__Result comparisons on UCMerced dataset with different methods__
![Comparison with the Other SR methods in UC](./figure/uc_result.png)

__Result comparisons on AID dataset with different methods__
![Comparison with the Other SR methods in AID](./figure/aid_result.png)

## Requirements
PyTorch >= 1.7
BasicSR == 1.4.2


### Installation
```
pip install -r requirements.txt
```

## How To Train
- Download the UCMerced dataset[[Baidu Drive](https://pan.baidu.com/s/1ijFUcLozP2wiHg14VBFYWw),password:terr][[Google Drive](https://drive.google.com/file/d/12pmtffUEAhbEAIn_pit8FxwcdNk4Bgjg/view)]and AID dataset[[Baidu Drive](https://pan.baidu.com/s/1Cf-J_YdcCB2avPEUZNBoCA),password:id1n][[Google Drive](https://drive.google.com/file/d/1d_Wq_U8DW-dOC3etvF4bbbWMOEqtZwF7/view)], they have been split them into train/val/test data, where the original images would be taken as the HR references and the corresponding LR images are generated by bicubic down-sample. 
- Refer to `./options/train` for the configuration file of the model to train.
- The training command is like
```
# x4
python basicsr/train.py -opt options/train/train_ACTSR_SRx4_AID_scratch --launcher pytorch
# x3
python basicsr/train.py -opt options/train/train_ACTSR_SRx4_AID_scratch --launcher pytorch
# x2
python basicsr/train.py -opt options/train/train_ACTSR_SRx4_AID_scratch --launcher pytorch

```
- Note that the default batch size per gpu is 12, which will cost about 22G memory for each GPU.

The training logs and weights will be saved in the `./experiments` folder.


## How To Test

- Refer to `./options/test` for the configuration file of the model to be tested, and prepare the testing data and pretrained model.  
- The pretrained models are available at
[Google Drive](https://drive.google.com/drive/folders/1qBIai0W-bsxpNKYbUiXpYQqDDVcDRgHH?usp=sharing) or [Baidu Netdisk](https://pan.baidu.com/s/1O-pTPH9Tdcfy17p4btNlFQ) (access code: rtc7).  
- Then run the follwing codes (taking `ACT_SRx4_AID.pth` as an example):
```
python basicsr/test.py -opt options/test/ACTSR_SRx4_AID.yml
```
The testing results will be saved in the `./results` folder.  



## Acknowledgements 
This code is built on [SwinIR (Pytorch)](https://github.com/JingyunLiang/SwinIR) and [HAT (Pytorch)](https://github.com/XPixelGroup/HAT). 


We thank the authors for sharing the codes.  